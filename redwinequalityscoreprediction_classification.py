# -*- coding: utf-8 -*-
"""RedWineQualityScorePrediction_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rPFn4FFJ7HdgWhltz8SUXm1koP8lsabI

#Library
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import sklearn.model_selection as model_selection
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import warnings
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import tree
import xgboost as xgb
import tensorflow as tf
from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense
from tensorflow.keras import datasets, layers, models
from keras.layers import Dropout,Dense,LSTM
from keras.models import Sequential
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_classification
from scipy import stats


import copy
import pickle

"""#Data Preparation

**Read data**
"""

wine_df = pd.read_csv('winequality-red.csv')


"""**convert to dataframe**"""

wine_df[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']] = pd.DataFrame(wine_df['fixed acidity;"volatile acidity";"citric acid";"residual sugar";"chlorides";"free sulfur dioxide";"total sulfur dioxide";"density";"pH";"sulphates";"alcohol";"quality"'].str.split(';', expand=True))
wine_df.drop('fixed acidity;"volatile acidity";"citric acid";"residual sugar";"chlorides";"free sulfur dioxide";"total sulfur dioxide";"density";"pH";"sulphates";"alcohol";"quality"', axis=1, inplace=True)


#Check Missing Values
print(wine_df.isna().sum())

"""**Convert type of variable from Object to Numberic**"""

for i in wine_df:
  wine_df[i] = pd.to_numeric(wine_df[i])


wine_df.info()

"""**สถิติของชุดข้อมูล dataset ที่ใช้**"""

wine_df.describe()

"""**Create Classification version of target variable**
 
ทำการแบ่ง เกรด ไวน์เป็น 3 เกรด โดยอิงจากค่า Quality

quality : 1-3 = wine_grade : 1

quality : 4-6 = wine_grade : 2

quality : 7ขึ้นไป = wine_grade : 3

"""

# Create Classification version of target variable
wine_df['wine_grade'] = [1 if x<=3 else 3 if x>=7 else 2 for x in wine_df['quality']]


wineGrade = wine_df[['quality','wine_grade']]


"""นับว่ามีไวน์แต่ละ grade กี่ตัว"""

wine_df['wine_grade'].value_counts()

"""**Histogram of quality**"""

fig = px.histogram(wine_df,x='quality')
fig.show()

"""แยก Data ออกจากกัน นำ feature variable เก็บไว้ใน X เพื่อที่จะนำไปเทรนต่อไป

และแยก target variable ที่เป็นคำตอบไว้ใน Y
"""

# Separate feature variables and target variable
X = pd.DataFrame(wine_df.drop(['quality','wine_grade'], axis = 1))
Y = pd.DataFrame(wine_df['wine_grade'])


"""**Correlation ระหว่าง input variables**"""

X_corr = X.corr()
lower = pd.DataFrame(np.tril(X_corr,-1),columns=X_corr.columns)


plt.subplots(figsize=(15,10))
sns.heatmap(lower, xticklabels=lower.columns, yticklabels=lower.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))

"""**Preparing Data for Modelling**

Standardizing Feature Variables
"""

standard_scaler = StandardScaler()
X_features = X
X = pd.DataFrame(standard_scaler.fit_transform(X_features), index = X.index, columns=X.columns )


"""Train-Test Split """

#Train-Test-Split 
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=6)


"""#Decision Tree"""

model_DT = DecisionTreeClassifier(criterion="entropy",random_state=10, splitter='best',max_depth = 5 ,min_samples_leaf = 10)
model_DT.fit(X_train, y_train)

y_pred_DT = model_DT.predict(X_test)
DTScore = accuracy_score(y_test, y_pred_DT)

print(DTScore)
print(confusion_matrix(y_test, y_pred_DT))
print(classification_report(y_test, y_pred_DT))

#Save Model Decision Tree
DecisionT_Model = 'DT_model_red'
pickle.dump(model_DT, open(DecisionT_Model, 'wb'))

select_feature = pd.DataFrame(wine_df.drop(['quality','wine_grade'], axis = 1))
X_feature = pd.DataFrame(X_train, columns = select_feature.columns)

feature_names = X_feature.columns
Labels = str(np.unique(y_train))
fig = plt.figure(figsize=(30,20))
tree.plot_tree(model_DT,
                      feature_names=feature_names,
                      class_names = Labels,
                      rounded = True,
                      filled = True,fontsize=9)
plt.show()

"""#KNN"""

k = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 25, 35]

KNNScore = pd.DataFrame(columns={'KNN_Score','k'})
for i in k :
  modelKNN = KNeighborsClassifier(n_neighbors=i, p=2)
  modelKNN.fit(X_train,y_train)
  Y_pred = modelKNN.predict(X_test)
  score = accuracy_score(y_test, Y_pred)
  add_score = pd.DataFrame([[i,score]],columns={'k','KNN_Score'})
  KNNScore = KNNScore.append(add_score)

fig = px.bar(KNNScore, x = 'k', y = 'KNN_Score', color='KNN_Score', range_y=[0.75,1.0])
fig.show()

# Print Confusion Matrix and Classification Report for best k
print('Confusion Matrix: ')
print(confusion_matrix(y_test, Y_pred))
print('Classification Report: ')
print(classification_report(y_test, Y_pred))

#Save Model KNN
kNN_Model = 'KNN_model_red'
pickle.dump(modelKNN, open(kNN_Model, 'wb'))

"""#Random Forest"""

ASM_function = ['entropy', 'gini'] 
nEstimator = 200
nJob = 2
randomState = 10

model_RF = RandomForestClassifier(criterion=ASM_function[1]
                                  ,n_estimators=nEstimator
                                  ,n_jobs=nJob
                                  ,random_state=randomState)

model_RF.fit(X_train, y_train)
y_pred_RF = model_RF.predict(X_test)

RFScore = accuracy_score(y_test,y_pred_RF)
print(RFScore)

print(confusion_matrix(y_test, y_pred_RF))
print(classification_report(y_test, y_pred_RF))

#Save Model RF
rF_Model = 'RF_model_red'
pickle.dump(model_RF, open(rF_Model, 'wb'))

feature_imp = pd.Series(model_RF.feature_importances_,index=feature_names).sort_values(ascending=False)

plt.figure(figsize=(15,15))
sns.barplot(x=feature_imp,y=feature_imp.index)

# Visualize selected estimator [0-5] tree structure of Random forest

#fig,axes = plt.subplots(nrows = 1,ncols = 5 , figsize = (10,2),dpi = 500)
#labels = str(np.unique(y_train))
#for index in range(0,5):
#  tree.plot_tree(model_RF.estimators_[index],
#                      feature_names = feature_names,
#                      class_names = labels,
#                      filled = True,
#                      ax = axes[index])
#  axes[index].set_title('Estimator: '+str(index),fontsize=11)

"""#LSTM"""

# Standardized data

df_feature = wine_df.drop(['quality','wine_grade'], axis = 1)
df_label = pd.DataFrame(wine_df['wine_grade'])

# Standardized data
standard_scaler = StandardScaler()
df_feature = pd.DataFrame(standard_scaler.fit_transform(df_feature), index = df_feature.index, columns=df_feature.columns )


df_label['wine_grade'].value_counts()

# ------------ Train-Test-Split 2D features -------------------------------
# set sliding window parameter
slidingW = 3 #จ ํานวน row
stride_step = 1
df_feature2D =[]
df_label_new = []
df_feature2D_T = []

for t in range(0,len(df_feature)-slidingW,stride_step):
  F2d= df_feature[t :( t + slidingW)]
  df_feature2D.append(F2d)
  F2d_T = np.transpose(F2d)
  df_feature2D_T.append(F2d_T)
  Labels = stats.mode(df_label.iloc[ t : ( t + slidingW) ]) # someting wrong here
  df_label_new.append(int(Labels[0][0]))

df_label_new = pd.DataFrame(df_label_new)

df_label_new.rename(columns={0: 'Label'},inplace = True)


df_label_new['Label'].value_counts()

df_label_new = pd.get_dummies(df_label_new, columns=['Label'] , drop_first=False)


# ------------ Train-Test-Split 2D features -------------------------------
x_train, x_test, y_train, y_test = train_test_split(np.array(df_feature2D) , np.array(df_label_new))

x_train_T, x_test_T, y_train_T, y_test_T = train_test_split(np.array(df_feature2D_T) , np.array(df_label_new))

# Nlayer (LSTM, dense), Nnode, Activation
LSTM_L1 = 100 # try 200, 300, 400, 500, 1000
LSTM_L2 = 50 # try 50, 100, 150, 200, 250, 300
dropRate_L1 = 0.1
dropRate_L2 = 0.25
D_out = 5
Activation = 'Softmax'
n_classes = 3


n_feature = 11
# try
#Option #1:
inRow = slidingW
inCol = n_feature
# Option #2
inRow_T = n_feature
inCol_T = slidingW

Input_shape = (inRow, inCol)
Input_shape_T = (inRow_T, inCol_T)

# ------------ Create LSTM Model -------------------------------
model = Sequential()
model.add(LSTM ( LSTM_L1, return_sequences=True,input_shape=Input_shape))

model.add(Dropout(dropRate_L1 ))
model.add(LSTM(LSTM_L2 ))
model.add(Dropout(dropRate_L2))
model.add(Dense(n_classes, activation='softmax'))
model.summary()

# ------------ Create LSTM Model Transpos -------------------------------
model_T = Sequential()
model_T.add( LSTM ( LSTM_L1, return_sequences=True,input_shape=Input_shape_T))
model_T.add(Dropout(dropRate_L1 ))
model_T.add(LSTM(LSTM_L2 ))
model_T.add(Dropout(dropRate_L2))
model_T.add(Dense(n_classes, activation='softmax'))
model_T.summary()

# ------------ Create Optimizer -------------------------------
model.compile( 
  optimizer='adam',
  loss='categorical_crossentropy',
  metrics=["acc"])

# ------------ Create Optimizer Transpos-------------------------------
model_T.compile( 
  optimizer='adam',
  loss='categorical_crossentropy',
  metrics=["acc"])

# ------ Train CNN using 2D feature--------------------------------------------
# Training the model
EP = 20
batch_size = 2 # try 20, 40, 60, 80, 100


history = model.fit( x_train, y_train, # try Option #1 และ Option #2
        batch_size = batch_size,
        validation_data = (x_test, y_test), epochs=EP)

# ------ Train CNN using 2DT feature--------------------------------------------
# Training the model Transpos
EP = 20
batch_size = 2 # try 20, 40, 60, 80, 100
history_T = model_T.fit( x_train_T, y_train_T, # try Option #1 และ Option #2
                      batch_size = batch_size,
                      validation_data=(x_test_T, y_test_T), epochs=EP)

LSTM_pred = model.predict(x_test)
LSTM_pred_T = model_T.predict(x_test_T)

x_test.shape

df_pred = pd.DataFrame(LSTM_pred)
df_pred = df_pred.idxmax(axis = 1)

df_y_real = pd.DataFrame(y_test)
df_y_real = df_y_real.idxmax(axis = 1)

#----------Transpos-------------
df_pred_T = pd.DataFrame(LSTM_pred_T)
df_class_T = df_pred_T.idxmax(axis=1)

df_y_real_T = pd.DataFrame(y_test_T)
df_y_real_T = df_y_real_T.idxmax(axis = 1)

print(confusion_matrix(df_y_real, df_pred))
print(classification_report(df_y_real_T,df_pred))

#----------Transpos-----------
print(confusion_matrix(df_y_real_T, df_class_T))
print(classification_report(df_y_real_T,df_class_T))

#Save Model LSTM
LSTM_Model = 'LSTM_model_red'
pickle.dump(model, open(LSTM_Model, 'wb'))

LSTM_T_Model = 'LSTM_T_model_red'
pickle.dump(model_T, open(LSTM_T_Model, 'wb'))

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.show()

plt.plot(history_T.history['acc'])
plt.plot(history_T.history['val_acc'])
plt.show()

plt.plot(history_T.history['loss'])
plt.plot(history_T.history['val_loss'])
plt.show()